{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Mechanistic Interpretability with SDialog\n",
    "\n",
    "In this tutorial, we will learn to use the mechanistic interpretability (M.I) feature from SDialog. We will mostly leverage the Inspector class from SDialog which is designed to be tailor-made for M.I .\n",
    "\n",
    "This tutorial is a reproduction of : Refusal in Language Models Is Mediated by a Single Direction (https://arxiv.org/pdf/2406.11717)\n",
    "\n",
    "Let's first define our model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from transformers.utils import logging as hf_logging\n",
    "from sdialog import Turn\n",
    "from sdialog.personas import Persona, PersonaAgent\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\" # Let's define our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a base agent. For the sake of simplicty, its persona definition will remain empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 20\n",
    "persona_agent = Persona() # Let's define a blank persona for this one.\n",
    "\n",
    "# Check if argument passing works as intended\n",
    "good_agent = PersonaAgent(persona=persona_agent, model=MODEL_NAME, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok ! Now, we need a set of harmful and harmless instructions. We can either fetch them from external datasets (using AdvBench and Alpaca), but the theory is that we could also generate them using a third agent :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_harmful_instructions():\n",
    "    url = 'https://raw.githubusercontent.com/SevKod/tutorials/refs/heads/main/datasets/refusal_direction/harmful_instructions.csv'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "    instructions = dataset['goal'].tolist()\n",
    "\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "def get_harmless_instructions():\n",
    "    url = 'https://raw.githubusercontent.com/SevKod/tutorials/refs/heads/main/datasets/refusal_direction/harmless_instructions.csv'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Read CSV content\n",
    "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "\n",
    "    # Extract instructions\n",
    "    instructions = dataset['instruction'].tolist()\n",
    "\n",
    "    # Split into train/test\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "\n",
    "print(\"Harmful instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(harmful_inst_train[i])}\") # The spooky instructions.\n",
    "print(\"Harmless instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(harmless_inst_train[i])}\") # The nice ones.\n",
    "\n",
    "harmful_inst_train = harmful_inst_train[:30]\n",
    "harmless_inst_train = harmless_inst_train[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! We are set. Time to apply the inspector to the agent. An Inspector is a specific class designed to be attached to an agent, and allow us to observe and intervene on the internal representations of the LLM attached to the agent.\n",
    "\n",
    "These allow us to alter the behavior of the LLM at a certain point in time.\n",
    "\n",
    "Let's first use the Inspector to gather these internal representations and use them to compute our direction. To scope the layers of interest, we define a dictionary where the key will represent the layer we want to inspect. The value will simply be the little name we want to give to the representations we extracted.\n",
    "\n",
    "To visualize the content of the llm, we need to access it through the agent, like this : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_agent.llm.llm.pipeline.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finding the best layer to compute the refusal direction is not easy, we need to find the right one. Empirical studies on this specific model find layer 16 to be the best one. Let's define it in our dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.interpretability import Inspector\n",
    "\n",
    "layer_name_to_key_agent = {\n",
    "    'model.layers.16.post_attention_layernorm': 'residual_post',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now let's define our inspector. We can pass our dictionary like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector_agent = Inspector(to_watch=layer_name_to_key_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good ! Now, the Inspector comes with a dunder-method (same as the Orchestration class), which makes it easy to attach to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_agent = good_agent | inspector_agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we attached many hooks at different places in the model, and we lost track of it. We can effectively see all the currently attached hooks through the recap() method of the inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector_agent.recap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now know that the inspector has a hook attached to layer 16 ! We also know that the agent has not spoken yet. \n",
    "\n",
    "We also know that no instructions were found in the agent. This is really important, because the inspector also allows us to track the instruction events during generation. This allows us to effectively generate a contrast through an instruction, and find where this instruction has been applied to get the representations.\n",
    "\n",
    "Let's stimulate our agent first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = good_agent(\"Hello ! How are you ?\") # Let's see how the agent responds to a simple question.\n",
    "print(output)\n",
    "output2 = good_agent(\"What is the capital of France ?\") # Let's see how the agent responds to a simple question.\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok good ! Now, let's look at how the inspector handles everything.\n",
    "\n",
    "So, the inspector is a class iterable that is hierarchically decomposed in : \n",
    "\n",
    "[i] : i-th utterance\n",
    "\n",
    "[i][k] : k-th token from the i-th utterance\n",
    "\n",
    "So far, our inspector has seen only two utterances for now. We can print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output of the first utterance : \",inspector_agent[0])\n",
    "print(\"Output of the second utterance : \",inspector_agent[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the number of utterances as well, using len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inspector_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the inspector has a built-in hook that is automatically attached to the tokenizer of the LLM, and that pre-computes the token equivalent of the string. This means that the inspector will allow you to observe each token individually and very simply. \n",
    "\n",
    "We can print the first token of the first utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspector_agent[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can count the number of tokens per utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inspector_agent[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that we previously attached one hook to observe the representations of layer 16. \n",
    "\n",
    "Well, we can simply fetch these representations for any token, by specifying the little value we attributed to the layer. \n",
    "\n",
    "In our case, it would be \"residual_post\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector_agent[0][0]['residual_post']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! It works ! We have all we need to compute the direction now !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, the author gather the representations on the first token generated. If you recall, we set it to 20  to showcase the features of the Inspector. But we could set it to 1 and speed up the computation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_representations = [] \n",
    "harmless_representations = []\n",
    "\n",
    "good_agent = PersonaAgent(persona=persona_agent, model=MODEL_NAME, max_new_tokens=2)\n",
    "\n",
    "# Harmful instructions loop\n",
    "for i in tqdm(range(len(harmful_inst_train)), desc=\"Computing harmful representations\", dynamic_ncols=True):\n",
    "    good_agent(harmful_inst_train[i])\n",
    "    harmful_representations.append(inspector_agent[0][0]['residual_post'])\n",
    "    good_agent.reset()\n",
    "\n",
    "# Harmless instructions loop\n",
    "for i in tqdm(range(len(harmless_inst_train)), desc=\"Computing harmless representations\", dynamic_ncols=True):\n",
    "    good_agent(harmless_inst_train[i])\n",
    "    harmless_representations.append(inspector_agent[0][0]['residual_post'])\n",
    "    good_agent.reset()\n",
    "\n",
    "# Concatenate and compute mean\n",
    "harmful_representations = torch.cat(harmful_representations, dim=0)\n",
    "harmless_representations = torch.cat(harmless_representations, dim=0)\n",
    "\n",
    "mean_harmful = harmful_representations.mean(dim=0)\n",
    "mean_harmless = harmless_representations.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the representations ! Now, we can compute the direction very easily by substracting the representations and normalizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the direction\n",
    "direction = mean_harmful - mean_harmless\n",
    "direction = direction / direction.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok ! We got the direction. Finally, we must steer all the layers to apply it and bypass the refusal. \n",
    "\n",
    "To do that, we simply need to substract the direction to the inspector. Internally, SDialog will call the function responsible for the substraction of the representations.\n",
    "\n",
    "Again, the layers to steer must be specified when defining the inspector. Here, we will choose to steer all the layers, using our direction isolated at layer 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name_to_steer = {}\n",
    "for i in range(28):\n",
    "    layer_name_to_steer[f'model.layers.{i}.post_attention_layernorm'] = f'pre_mlp_{i}'\n",
    "    layer_name_to_steer[f'model.layers.{i}.mlp'] = f'post_mlp_{i}'\n",
    "    layer_name_to_steer[f'model.layers.{i}'] = f'residual_post_{i}'\n",
    "\n",
    "intruder= Inspector(to_watch=layer_name_to_steer)\n",
    "\n",
    "persona_agent = Persona()\n",
    "\n",
    "max_new_tokens = 80\n",
    "\n",
    "agent_steered = PersonaAgent(persona=persona_agent, model=MODEL_NAME, max_new_tokens=max_new_tokens)\n",
    "agent_steered.memory = agent_steered.memory[1:] # Quick way to truncate the memory\n",
    "agent_steered = agent_steered | intruder - direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to use our steered agent and see how our boy now behaves with harmful request !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_output = []\n",
    "agent_steered_output = []\n",
    "\n",
    "nb_of_instructions_to_steer = 10\n",
    "\n",
    "baseline_agent = PersonaAgent(persona=persona_agent, model=MODEL_NAME, max_new_tokens=max_new_tokens)\n",
    "\n",
    "for i in tqdm(range(len(harmful_inst_test[:nb_of_instructions_to_steer])), desc=\"Generating harmful responses 💀:\", dynamic_ncols=True):\n",
    "    output = agent_steered(harmful_inst_test[i])\n",
    "    agent_steered_output.append(output)\n",
    "    \n",
    "    output = baseline_agent(harmful_inst_test[i])\n",
    "    baseline_output.append(output)\n",
    "\n",
    "    agent_steered.reset()\n",
    "    baseline_agent.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore\n",
    "import textwrap\n",
    "\n",
    "for i in range(len(harmful_inst_test[:50])):\n",
    "    print(f\"INSTRUCTION {i}: {repr(harmful_inst_test[i])}\")\n",
    "    \n",
    "    print(Fore.GREEN + \"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_output[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    \n",
    "    print(Fore.RED + \"AGENT STEERED COMPLETION:\")\n",
    "    print(textwrap.fill(repr(agent_steered_output[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RESET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.llm.llm.pipeline.model.model[0].post_attention_layernorm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsalt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
